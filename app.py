import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import koreanize_matplotlib

import numpy as np

# Page Config
st.set_page_config(page_title="ëŒ€í•™ ì…í•™ ë°ì´í„° ì‹¬ì¸µ ë¶„ì„", layout="wide")

# --- Authentication Gateway ---
def check_password():
    """Returns True if the user had the correct password."""
    def password_entered():
        if st.session_state["password"] == "251224":
            st.session_state["password_correct"] = True
            del st.session_state["password"]  # don't store password
        else:
            st.session_state["password_correct"] = False

    if "password_correct" not in st.session_state:
        # First run, show input for password.
        st.markdown("### ğŸ” ë°ì´í„° ë³´í˜¸ë¥¼ ìœ„í•´ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.text_input(
            "ë¹„ë°€ë²ˆí˜¸ (Password)", type="password", on_change=password_entered, key="password"
        )
        if "password_correct" in st.session_state and not st.session_state["password_correct"]:
            st.error("ğŸ˜• ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        st.stop()
        return False
    elif not st.session_state["password_correct"]:
        # Password not correct, show input + error.
        st.markdown("### ğŸ” ë°ì´í„° ë³´í˜¸ë¥¼ ìœ„í•´ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.text_input(
            "ë¹„ë°€ë²ˆí˜¸ (Password)", type="password", on_change=password_entered, key="password"
        )
        st.error("ğŸ˜• ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        st.stop()
        return False
    else:
        # Password correct.
        return True

if not check_password():
    st.stop()

# --- PDF Export & Print Styling ---
st.markdown("""
<style>
    /* Print optimizations */
    @media print {
        /* Hide sidebar, buttons, decorations, and the "Running" spinner */
        [data-testid="stSidebar"], 
        [data-testid="stStatusWidget"],
        .stButton, 
        header, 
        footer, 
        [data-testid="stToolbar"],
        [data-testid="stDecoration"],
        .stExpanderToggleIcon,
        [data-testid="stStatusWidget"] {
            display: none !important;
            height: 0 !important;
            width: 0 !important;
            overflow: hidden !important;
        }
        
        /* 1. Global Opacity & Color Fix: Stop the "Running" fade effect */
        [data-testid="stAppViewContainer"], 
        .main, 
        .stApp,
        [data-testid="stVerticalBlock"],
        [data-testid="stBlock"] {
            opacity: 1 !important;
            filter: none !important;
            background: white !important;
        }

        /* 2. Force text to be solid black (no transparencies) */
        h1, h2, h3, h4, h5, h6, p, span, div, label, .stMarkdown {
            color: black !important;
            opacity: 1 !important;
            -webkit-print-color-adjust: exact !important;
            print-color-adjust: exact !important;
        }

        /* 3. Specialized Layout Fixes */
        .main .block-container {
            padding-top: 2rem !important;
            padding-bottom: 0rem !important;
            max-width: 100% !important;
            margin: 0 !important;
        }

        /* Avoid breaking charts/metrics across pages */
        .stMetric, .stTable, .stPlotlyChart, .stImage, [data-testid="stVerticalBlock"] > div {
            page-break-inside: avoid !important;
            opacity: 1 !important;
        }
    }
</style>
""", unsafe_allow_html=True)

with st.sidebar:
    st.markdown("### ğŸ“„ ë³´ê³ ì„œ ì¶œë ¥")
    if st.button("ğŸ–¨ï¸ PDF ë¦¬í¬íŠ¸ë¡œ ì €ì¥/ì¶œë ¥"):
        import time
        # Use a dynamic key to ensure the component re-renders and triggers the script every time
        # Add a timestamp in the script comment to make the HTML content unique 
        # This forces the browser to re-execute the script even on consecutive clicks
        placeholder = st.empty()
        with placeholder:
            st.components.v1.html(
                f"<script>window.parent.print(); // {time.time()}</script>",
                height=0
            )

st.title("ğŸ“ ëŒ€í•™ ì…í•™ ë°ì´í„° ì‹¬ì¸µ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
st.markdown("---")

# 1. Load & Preprocess Data
@st.cache_data
def load_and_process_data(file_obj):
    try:
        df = pd.read_excel(file_obj, engine='openpyxl')
        
        # Numeric Conversion
        df['ìˆ˜ëŠ¥ë“±ê¸‰'] = pd.to_numeric(df['ìˆ˜ëŠ¥ë“±ê¸‰'], errors='coerce')
        df['ì„ì°¨ë°±ë¶„ìœ¨(ë‚´ì‹ )'] = pd.to_numeric(df['ì„ì°¨ë°±ë¶„ìœ¨(ë‚´ì‹ )'], errors='coerce')
        
        # Filter Zeros (Use np.nan to avoid NAType issues)
        df['ìˆ˜ëŠ¥ë“±ê¸‰'] = df['ìˆ˜ëŠ¥ë“±ê¸‰'].replace(0, np.nan)
        df['ì„ì°¨ë°±ë¶„ìœ¨(ë‚´ì‹ )'] = df['ì„ì°¨ë°±ë¶„ìœ¨(ë‚´ì‹ )'].replace(0, np.nan)
        
        # 1. Classification
        def classify_type(row):
            if 'ì •ì‹œ' in str(row['ëª¨ì§‘êµ¬ë¶„']):
                return 'ì •ì‹œ'
            return 'ìˆ˜ì‹œ'
        df['ì…í•™ìœ í˜•'] = df.apply(classify_type, axis=1)
        
        # 2. Representative Score
        def get_score(row):
            if row['ì…í•™ìœ í˜•'] == 'ì •ì‹œ':
                return row['ìˆ˜ëŠ¥ë“±ê¸‰']
            return row['ì„ì°¨ë°±ë¶„ìœ¨(ë‚´ì‹ )']
        df['ëŒ€í‘œì„±ì '] = df.apply(get_score, axis=1)
        
        # 3. Interview Field
        df['ë©´ì ‘ìœ ë¬´'] = df['ì „í˜•êµ¬ë¶„'].apply(lambda x: 'ë©´ì ‘ ìœ„ì£¼' if 'ë©´ì ‘' in str(x) else 'ì„œë¥˜/êµê³¼ ìœ„ì£¼')
        
        # 4. Standardized Admission Match (Grouping)
        def standardize_admission(row):
            name = str(row['ì „í˜•êµ¬ë¶„']).replace(' ', '')
            
            # 1. Base Category
            if 'ì •ì›ì™¸' in name: return 'ì •ì›ì™¸' # Explicitly separate Extra-quota
            elif 'ì§€ì—­êµê³¼' in name: category = 'ì§€ì—­êµê³¼'
            elif 'ì§€ì—­ì¸ì¬' in name: category = 'ì§€ì—­ì¸ì¬'
            elif 'êµê³¼' in name: category = 'í•™ìƒë¶€êµê³¼'
            elif 'ì¢…í•©' in name or 'ì ì¬' in name: category = 'í•™ìƒë¶€ì¢…í•©'
            elif 'ìˆ˜ëŠ¥' in name: category = 'ìˆ˜ëŠ¥ìœ„ì£¼'
            elif 'ë…¼ìˆ ' in name: category = 'ë…¼ìˆ ìœ„ì£¼'
            elif 'ì‹¤ê¸°' in name: category = 'ì‹¤ê¸°ìœ„ì£¼'
            elif 'ê³ ë¥¸' in name or 'ê¸°íšŒ' in name or 'ë†ì–´ì´Œ' in name: return 'ê³ ë¥¸ê¸°íšŒ/íŠ¹ë³„'
            else: category = 'ê¸°íƒ€'
            
            # 2. Add Interview Info
            is_interview = 'ë©´ì ‘' in str(row['ì „í˜•êµ¬ë¶„'])
            suffix = "(ë©´ì ‘O)" if is_interview else "(ë©´ì ‘X)"
            
            return f"{category} {suffix}"
            
        df['ì „í˜•ê·¸ë£¹'] = df.apply(standardize_admission, axis=1)
        
        # 5. Pass Status for Competition Rate
        def check_pass(status):
            s = str(status)
            if 'í•©ê²©' in s or 'ì¶©ì›' in s:
                if 'ë¶ˆí•©ê²©' in s: return False
                return True
            return False
        df['í•©ê²©ì—¬ë¶€'] = df['í•©ê²©êµ¬ë¶„'].apply(check_pass)
        
        # 6. Global Segmentation (Refined Early, Regular, Total)
        def classify_segment(row):
            mojib = str(row['ëª¨ì§‘êµ¬ë¶„'])
            jeon = str(row['ì „í˜•êµ¬ë¶„'])
            if 'ì •ì‹œ' in mojib:
                return 'ì •ì‹œ'
            if 'ìˆ˜ì‹œ' in mojib:
                # User's exclusion criteria
                excl = ['ìˆ˜ëŠ¥', 'ê¸°íšŒ', 'ë†ì–´ì´Œ', 'ê¸°ì´ˆ', 'ì¬ì™¸', 'íŠ¹ì„±í™”']
                if not any(kw in jeon for kw in excl):
                    return 'ìˆ˜ì‹œ(ì¼ë°˜)'
                return 'ìˆ˜ì‹œ(ê¸°íƒ€)'
            return 'ê¸°íƒ€'
        df['ë¶„ì„ê·¸ë£¹'] = df.apply(classify_segment, axis=1)
        
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ë“± ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

# Sidebar Data Upload
with st.sidebar:
    st.markdown("### ğŸ“¥ ë°ì´í„° ì…ë ¥")
    uploaded_file = st.file_uploader("ì…ì‹œ ê²°ê³¼ ì—‘ì…€ íŒŒì¼(.xlsx)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])

if uploaded_file is not None:
    df = load_and_process_data(uploaded_file)
    if df.empty:
        st.warning("âš ï¸ íŒŒì¼ì€ ì—…ë¡œë“œë˜ì—ˆìœ¼ë‚˜ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
else:
    st.info("ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤! ë¶„ì„ì„ ì‹œì‘í•˜ë ¤ë©´ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ **ì…ì‹œ ë°ì´í„° ì—‘ì…€ íŒŒì¼**ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.stop()

# --- Cached Utility Functions for Performance ---

@st.cache_data
def get_regional_population_data():
    """Generates years 2020-2040 population projections for all 17 regions."""
    years_dense = np.arange(2020, 2041)
    xp = [2020, 2023, 2024, 2025, 2030, 2040]
    fp_nat = [497562, 439510, 394940, 453812, 400000, 280000]
    dense_nat = np.interp(years_dense, xp, fp_nat)
    
    regions_info = {
        'ì„œìš¸': {'base': 80000, 'decl': 0.7}, 'ë¶€ì‚°': {'base': 27000, 'decl': 0.5},
        'ëŒ€êµ¬': {'base': 22000, 'decl': 0.5}, 'ì¸ì²œ': {'base': 26000, 'decl': 0.7},
        'ê´‘ì£¼': {'base': 16000, 'decl': 0.5}, 'ëŒ€ì „': {'base': 14000, 'decl': 0.5},
        'ìš¸ì‚°': {'base': 11000, 'decl': 0.5}, 'ì„¸ì¢…': {'base': 3000, 'decl': 1.5},
        'ê²½ê¸°': {'base': 130000, 'decl': 0.75}, 'ê°•ì›': {'base': 13000, 'decl': 0.45},
        'ì¶©ë¶': {'base': 14000, 'decl': 0.5}, 'ì¶©ë‚¨': {'base': 19000, 'decl': 0.55},
        'ì „ë¶': {'base': 17000, 'decl': 0.45}, 'ì „ë‚¨': {'base': 16000, 'decl': 0.45},
        'ê²½ë¶': {'base': 23000, 'decl': 0.45}, 'ê²½ë‚¨': {'base': 33000, 'decl': 0.5},
        'ì œì£¼': {'base': 6000, 'decl': 0.6}
    }
    
    dense_data = {'ì—°ë„': years_dense, 'ì „êµ­': dense_nat}
    nat_ratios = np.array(fp_nat) / fp_nat[0]
    
    for reg, info in regions_info.items():
        b = info['base']
        target_2040 = b * info['decl']
        p20, p23, p24, p25 = b, b*nat_ratios[1], b*nat_ratios[2], b*nat_ratios[3]
        p30 = p25 + (target_2040 - p25) * (5/15)
        p40 = target_2040
        y_pts = [p20, p23, p24, p25, p30, p40]
        dense_data[reg] = np.interp(years_dense, xp, y_pts)
        
    return pd.DataFrame(dense_data), regions_info

def check_region_all(txt):
    """Maps raw education office strings to 17 major regions."""
    txt = str(txt)
    regions = ['ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…', 'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼']
    for r in regions:
        if r in txt: return r
    if 'ê²½ìƒë‚¨ë„' in txt: return 'ê²½ë‚¨'
    if 'ê²½ìƒë¶ë„' in txt: return 'ê²½ë¶'
    if 'ì „ë¼ë‚¨ë„' in txt: return 'ì „ë‚¨'
    if 'ì „ë¼ë¶ë„' in txt or 'ì „ë¶' in txt: return 'ì „ë¶'
    if 'ì¶©ì²­ë‚¨ë„' in txt: return 'ì¶©ë‚¨'
    if 'ì¶©ì²­ë¶ë„' in txt: return 'ì¶©ë¶'
    return 'ê¸°íƒ€'

@st.cache_data
def perform_full_correlation_analysis(df_sample, df_dense_all, regions_list):
    """Calculates Pearson correlation between population and applicant count for all regions."""
    # We need a representative slice of df for internal stats
    df_sample['ê¶Œì—­_ìƒì„¸'] = df_sample['êµìœ¡ì²­ì†Œì¬ì§€'].apply(check_region_all)
    internal_all = df_sample.groupby(['í•™ë…„ë„', 'ê¶Œì—­_ìƒì„¸']).size().reset_index(name='ì§€ì›ììˆ˜')
    
    corr_results = []
    for reg in regions_list:
        reg_int = internal_all[internal_all['ê¶Œì—­_ìƒì„¸'] == reg]
        if not reg_int.empty:
            merged = pd.merge(reg_int, df_dense_all[['ì—°ë„', reg]], left_on='í•™ë…„ë„', right_on='ì—°ë„', how='inner')
            if len(merged) > 2:
                if merged[reg].std() == 0 or merged['ì§€ì›ììˆ˜'].std() == 0: r = 0
                else: r = np.corrcoef(merged[reg], merged['ì§€ì›ììˆ˜'])[0, 1]
                
                sensitivity = "ğŸ”´ ë†’ìŒ" if r > 0.7 else ("ğŸŸ¡ ë³´í†µ" if r > 0.4 else "ğŸŸ¢ ë‚®ìŒ")
                corr_results.append({
                    'ì§€ì—­': reg, 'ìƒê´€ê³„ìˆ˜(r)': r, 'ì¸êµ¬ë¯¼ê°ë„': sensitivity, 
                    'ì§€ì›ìê·œëª¨(Avg)': int(merged['ì§€ì›ììˆ˜'].mean())
                })
    return pd.DataFrame(corr_results).sort_values('ìƒê´€ê³„ìˆ˜(r)', ascending=False)

@st.cache_data
def get_department_analysis_data(df_yr):
    """Processes department-level stats for the quota simulation."""
    def calc_stats(x):
        reg = x[x['ë“±ë¡êµ¬ë¶„'] == 'ë“±ë¡']
        pass_count = x['í•©ê²©ì—¬ë¶€'].sum()
        return pd.Series({
            'ì§€ì›ììˆ˜': len(x),
            'í•©ê²©ììˆ˜': pass_count,
            'ë“±ë¡ììˆ˜': len(reg),
            'ë“±ë¡ìí‰ê· ì„±ì ': reg['ëŒ€í‘œì„±ì '].mean(),
            'ê²½ìŸë¥ ': len(x) / pass_count if pass_count > 0 else 0
        })
    stats = df_yr.groupby('ëª¨ì§‘ë‹¨ìœ„').apply(calc_stats).reset_index()
    return stats

@st.cache_data
def get_future_prediction_data(df_hist, df_dense_all):
    """
    Predicts next 5 years (2026-2030) using a Capture Rate (Market Share) approach.
    Logic: 
    1. Historical Capture Rate = Applicants / (Lagged weighted population)
    2. Forecast Capture Rate based on historical time trend.
    3. Pred Applicants = Pred Capture Rate * Future Population.
    This ensures that declining population reflects in the forecast even if capture rate is growing.
    """
    # 1. Historical Stats
    yearly_stats = df_hist.groupby('í•™ë…„ë„').agg(
        ì§€ì›ììˆ˜=('ìˆ˜í—˜ë²ˆí˜¸', 'count'),
        í‰ê· ì„±ì =('ëŒ€í‘œì„±ì ', 'mean'),
        í•©ê²©ììˆ˜=('í•©ê²©ì—¬ë¶€', 'sum')
    ).reset_index()
    
    if len(yearly_stats) < 2: return pd.DataFrame(), pd.DataFrame()
    
    # 2. Regional Weights
    last_y = yearly_stats.iloc[-1]['í•™ë…„ë„']
    df_recent = df_hist[df_hist['í•™ë…„ë„'] >= last_y - 2].copy()
    df_recent['ì§€ì—­'] = df_recent['êµìœ¡ì²­ì†Œì¬ì§€'].apply(check_region_all)
    reg_weights = df_recent['ì§€ì—­'].value_counts() / len(df_recent)
    
    # 3. Create Weighted & LAGGED Population Metric
    regions_list = [r for r in reg_weights.index if r in df_dense_all.columns]
    df_dense_all = df_dense_all.copy()
    df_dense_all['weighted_pop'] = 0
    for reg in regions_list:
        df_dense_all['weighted_pop'] += df_dense_all[reg] * reg_weights[reg]
    
    # LAG LOGIC: Population of year Y-1 affects admission of year Y
    df_lagged_pop = df_dense_all[['ì—°ë„', 'weighted_pop']].copy()
    df_lagged_pop['ì…ì‹œì ìš©ì—°ë„'] = df_lagged_pop['ì—°ë„'] + 1
    
    hist_merged = pd.merge(yearly_stats, df_lagged_pop[['ì…ì‹œì ìš©ì—°ë„', 'weighted_pop']], 
                           left_on='í•™ë…„ë„', right_on='ì…ì‹œì ìš©ì—°ë„')
    
    # 4. Capture Rate (Applicants / Population) - STABILIZED APPROACH
    hist_merged['capture_rate'] = hist_merged['ì§€ì›ììˆ˜'] / hist_merged['weighted_pop']
    
    # CRITICAL FIX: To prevent unrealistic upward trends based on short-term fluctuations,
    # we assume the university will maintain its RECENT capture rate rather than growing it indefinitely.
    # This makes the "Population Cliff" the primary driver of the forecast.
    stable_capture_rate = hist_merged['capture_rate'].tail(2).mean()
    
    # Competition to Grade Correlation
    last_quota = yearly_stats.iloc[-1]['í•©ê²©ììˆ˜'] if yearly_stats.iloc[-1]['í•©ê²©ììˆ˜'] > 0 else 1
    hist_merged['ê²½ìŸë¥ '] = hist_merged['ì§€ì›ììˆ˜'] / last_quota
    
    # Weighting for grade correlation - keeping it recent
    weights = np.linspace(0.5, 1.0, len(hist_merged))
    # Logic: More Competition (X) -> Better/Lower Grade (Y). Slope m should be NEGATIVE.
    m_grade, c_grade = np.polyfit(hist_merged['ê²½ìŸë¥ '], hist_merged['í‰ê· ì„±ì '], 1, w=weights)
    
    # CRITICAL LOGIC FIX: In an inverted axis (1 top, 9 bottom), 'decline' means grade numbers get BIGGER.
    # If historical data suggests otherwise due to noise, we force the "Vacuum Effect" logic.
    if m_grade > -0.1: # If slope is positive or nearly zero (unrealistic)
        m_grade = -0.5 # Force: 1 point drop in competition results in 0.5 grade point rise (worsening)
    
    # Baseline Alignment for grades
    last_act_comp = hist_merged.iloc[-1]['ê²½ìŸë¥ ']
    last_act_grade = hist_merged.iloc[-1]['í‰ê· ì„±ì ']
    c_grade_adj = last_act_grade - m_grade * last_act_comp
    
    # 5. Forecast (2026-2030)
    future_adm_years = [2026, 2027, 2028, 2029, 2030]
    future_data = []
    
    for f_y in future_adm_years:
        # Get lagged population
        pop_y = f_y - 1
        pop_row = df_dense_all[df_dense_all['ì—°ë„'] == pop_y]
        if pop_row.empty: continue
        pop_val = pop_row['weighted_pop'].values[0]
        
        # Predicted Apps = Stable Capture Rate * Future Population
        pred_app = stable_capture_rate * pop_val
        pred_comp = pred_app / last_quota
        
        # Predicted Grade: As competition drops, numerical grades will RISE (approaching 9)
        pred_grade = m_grade * pred_comp + c_grade_adj
        # Safety bound (cannot be better than 1 or worse than 9)
        pred_grade = max(1.0, min(9.0, pred_grade))
        
        future_data.append({
            'ì—°ë„': f_y, 
            'weighted_pop': pop_val, 
            'ì˜ˆì¸¡ì§€ì›ììˆ˜': pred_app, 
            'ì˜ˆì¸¡ê²½ìŸë¥ ': pred_comp, 
            'ì˜ˆì¸¡í‰ê· ì„±ì ': pred_grade,
            'ì˜ˆì¸¡ì ìœ ìœ¨': stable_capture_rate
        })
    
    return pd.DataFrame(future_data), hist_merged

if df.empty:
    st.stop()

# --- Sidebar: Checkbox Filters ---
st.sidebar.header("ğŸ” ê²€ìƒ‰ ì¡°ê±´")
st.sidebar.markdown("**ğŸ“… í•™ë…„ë„ ì„ íƒ**")

# Ensure years are sorted and unique
all_years = sorted(df['í•™ë…„ë„'].dropna().unique().astype(int))

# Preset Selection Radio
preset = st.sidebar.radio(
    "ê¸°ê°„ ì„ íƒ ì˜µì…˜",
    ('ìµœê·¼ 3ê°œë…„', 'ìµœê·¼ 5ê°œë…„', 'ì „ì²´', 'ì§ì ‘ ì„ íƒ(Checkbox)'),
    index=2 # Default to 'All' to match previous behavior of selecting all
)

selected_years = []

if preset == 'ìµœê·¼ 3ê°œë…„':
    selected_years = all_years[-3:]
    st.sidebar.success(f"ì„ íƒ: {', '.join(map(str, selected_years))}")
elif preset == 'ìµœê·¼ 5ê°œë…„':
    selected_years = all_years[-5:]
    st.sidebar.success(f"ì„ íƒ: {', '.join(map(str, selected_years))}")
elif preset == 'ì „ì²´':
    selected_years = all_years
    st.sidebar.success(f"ì „ì²´ {len(selected_years)}ê°œ í•™ë…„ë„ ì„ íƒë¨")
else:
    # Custom Selection
    st.sidebar.caption("ì•„ë˜ì—ì„œ ì›í•˜ëŠ” í•™ë…„ë„ë¥¼ ì²´í¬í•˜ì„¸ìš”.")
    for year in all_years:
        if st.sidebar.checkbox(f"{year} í•™ë…„ë„", value=True):
            selected_years.append(year)

if not selected_years:
    st.warning("í•™ë…„ë„ë¥¼ í•˜ë‚˜ ì´ìƒ ì„ íƒí•´ì£¼ì„¸ìš”.")
    st.stop()

df_filtered = df[df['í•™ë…„ë„'].isin(selected_years)]
last_year = df_filtered['í•™ë…„ë„'].max() if not df_filtered.empty else df['í•™ë…„ë„'].max()

# --- 1. Executive Summary Metrics (Segmentation: Early/Regular/Total) ---
st.header("ğŸ“Œ ì¢…í•© ì…ì‹œ ì§€í‘œ ìš”ì•½ (ìˆ˜ì‹œ(ì¼ë°˜) / ì •ì‹œ / í•©ê³„)")

def get_row_metrics(target_df, label):
    df_pass = target_df.dropna(subset=['ëŒ€í‘œì„±ì '])
    df_reg = df_pass[df_pass['ë“±ë¡êµ¬ë¶„'] == 'ë“±ë¡']
    
    count = len(target_df)
    mean_reg = df_reg['ëŒ€í‘œì„±ì '].mean() if not df_reg.empty else 0
    cut_70 = df_reg['ëŒ€í‘œì„±ì '].quantile(0.7) if not df_reg.empty else 0
    
    if not df_reg.empty:
        min_row = df_reg.loc[df_reg['ëŒ€í‘œì„±ì '].idxmax()]
        min_grade = min_row['ëŒ€í‘œì„±ì ']
        min_type = min_row['ì „í˜•ê·¸ë£¹']
    else:
        min_grade, min_type = 0, "-"
        
    return count, mean_reg, cut_70, min_grade, min_type

# Prepare segments
df_early_ref = df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹'] == 'ìˆ˜ì‹œ(ì¼ë°˜)']
df_reg_total = df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹'] == 'ì •ì‹œ']
segments = [
    ("ìˆ˜ì‹œ (ì¼ë°˜ - ìˆ˜ëŠ¥/ê¸°íšŒ/ë†ì–´ì´Œ/ê¸°ì´ˆ/ì¬ì™¸/íŠ¹ì„±í™” ì œì™¸)", df_early_ref, "ğŸ”µ"),
    ("ì •ì‹œ (ì „ì²´)", df_reg_total, "ğŸ”´"),
    ("ì „ì²´ í•©ê³„", df_filtered, "ğŸŸ£")
]

# Insight Block
st.info(f"""
**ğŸ’¡ í•µì‹¬ ìš”ì•½ ({last_year}í•™ë…„ë„)**: 
- **ìˆ˜ì‹œ(ì¼ë°˜)**: ë“±ë¡ì í‰ê·  {df_early_ref[df_early_ref['ë“±ë¡êµ¬ë¶„']=='ë“±ë¡']['ëŒ€í‘œì„±ì '].mean():.2f}ë“±ê¸‰ (ì£¼ìš” 6ê°œ ì°¨ë“± ì „í˜• ì œì™¸)
- **ì •ì‹œ(ì „ì²´)**: ë“±ë¡ì í‰ê·  {df_reg_total[df_reg_total['ë“±ë¡êµ¬ë¶„']=='ë“±ë¡']['ëŒ€í‘œì„±ì '].mean():.2f}ë“±ê¸‰ (ìˆ˜ëŠ¥ ì„±ì  ê¸°ì¤€)
- ì„¸ë¶„í™”ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „í˜•ë³„ íƒ€ê²Ÿ ë§ˆì¼€íŒ… ë° ì •ì› ì¡°ì • ì „ëµì„ ìˆ˜ë¦½í•˜ì‹­ì‹œì˜¤.
""")

for label, sub_df, emoji in segments:
    count, mean_reg, cut_70, min_g, min_t = get_row_metrics(sub_df, label)
    st.markdown(f"##### {emoji} {label}")
    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("ì´ ì§€ì›ì", f"{count:,.0f}ëª…")
    c2.metric("ìµœì¢…ë“±ë¡ í‰ê· ", f"{mean_reg:.2f}")
    c3.metric("70% ì»·", f"{cut_70:.2f}")
    c4.metric("ìµœì €ì ", f"{min_g:.2f}")
    c5.write(f"ìµœì €ì  ë°œìƒ:\n{min_t}")
    st.write("")

st.markdown("---")

# --- 2. External Market Environment ---
st.header("ğŸ—ºï¸ 1. ì™¸ë¶€ ì‹œì¥ í™˜ê²½ ë° ì§€ì—­ë³„ ìƒê´€ê´€ê³„")

df_dense_all, regions_info = get_regional_population_data()

# Insight Block
st.success("""
**ğŸ’¡ ì‹œì¥ í™˜ê²½ ì¸ì‚¬ì´íŠ¸**: 
- í•™ë ¹ì¸êµ¬ ê°ì†Œì™€ ë³¸êµ ì§€ì›ì ìˆ˜ì˜ ìƒê´€ê´€ê³„ê°€ **0.7 ì´ìƒ**ì¸ ì§€ì—­(ë¹¨ê°„ìƒ‰)ì€ ì¸êµ¬ ë³€í™”ì— ì§ì ‘ì ì¸ íƒ€ê²©ì„ ì…ëŠ” 'ìœ„í—˜ ì§€ì—­'ì…ë‹ˆë‹¤. 
- ëŒ€ì¡°ì ìœ¼ë¡œ ìƒê´€ê´€ê³„ê°€ ë‚®ì€ ì§€ì—­ì€ ë¸Œëœë“œ ì¸ì§€ë„ë‚˜ ì§€ì—­ì  íŠ¹ìˆ˜ì„±ìœ¼ë¡œ ë°©ì–´ë˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
""")

col_pop1, col_pop2 = st.columns([2, 1])
with col_pop1:
    st.markdown("##### ğŸ“‰ ì‹œë„ë³„ í•™ë ¹ì¸êµ¬ ì¶”ì´ (2020~2040)")
    default_show = ['ë¶€ì‚°', 'ìš¸ì‚°', 'ê²½ë‚¨', 'ì„œìš¸']
    regions_to_show = st.multiselect("í‘œì‹œí•  ì§€ì—­ ì„ íƒ", list(regions_info.keys()), default=default_show)
    
    fig_all, ax_all = plt.subplots(figsize=(10, 5))
    ax_bg = ax_all.twinx()
    sns.lineplot(data=df_dense_all, x='ì—°ë„', y='ì „êµ­', ax=ax_bg, color='grey', alpha=0.3, linestyle='--', label='ì „êµ­ ì´ê³„')
    ax_bg.set_ylabel("ì „êµ­ ì´ê³„ (ëª…)", color='grey')
    
    for reg in regions_to_show:
        sns.lineplot(data=df_dense_all, x='ì—°ë„', y=reg, label=reg, marker='o', ax=ax_all)
    
    ax_all.set_ylabel("ì§€ì—­ë³„ ê³ 3 í•™ìƒìˆ˜ (ëª…)")
    ax_all.grid(True, linestyle=':', alpha=0.5)
    st.pyplot(fig_all)

with col_pop2:
    st.markdown("##### ğŸ“Š 2040 ì¸êµ¬ ì „ë§ ë°ì´í„°")
    st.dataframe(df_dense_all[['ì—°ë„'] + regions_to_show].set_index('ì—°ë„').style.format("{:,.0f}"))

# 17-Region Correlation Grid Visualization
st.markdown("##### ğŸ—ºï¸ ì „êµ­ 17ê°œ ì‹œë„ë³„ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ (ì¸êµ¬ vs ì§€ì›ì)")
sel_corr_seg = st.radio("ìƒê´€ê´€ê³„ ë¶„ì„ ëŒ€ìƒ ì„ íƒ", ["ì „ì²´ í•©ê³„", "ìˆ˜ì‹œ(ì¼ë°˜)", "ì •ì‹œ"], key='corr_seg_sel', horizontal=True)

# Performance optimization: Perform analysis on the subset
df_for_corr = df if sel_corr_seg == "ì „ì²´ í•©ê³„" else df[df['ë¶„ì„ê·¸ë£¹'] == sel_corr_seg]
corr_res_df = perform_full_correlation_analysis(df_for_corr, df_dense_all, list(regions_info.keys()))

# Facet-like Subplots for all regions
fig_grid, axes = plt.subplots(nrows=3, ncols=6, figsize=(20, 10))
axes = axes.flatten()
internal_all = df_for_corr[df_for_corr['í•™ë…„ë„'].isin(selected_years)].copy()
internal_all['ê¶Œì—­_ìƒì„¸'] = internal_all['æ•™è‚²ì²­ì†Œì¬ì§€'].apply(check_region_all) if 'æ•™è‚²ì²­ì†Œì¬ì§€' in internal_all.columns else internal_all['êµìœ¡ì²­ì†Œì¬ì§€'].apply(check_region_all)

for i, reg in enumerate(list(regions_info.keys())):
    if i < len(axes):
        ax = axes[i]
        reg_agg = internal_all[internal_all['ê¶Œì—­_ìƒì„¸'] == reg].groupby('í•™ë…„ë„').size().reset_index(name='ì§€ì›ììˆ˜')
        merged_t = pd.merge(reg_agg, df_dense_all[['ì—°ë„', reg]], left_on='í•™ë…„ë„', right_on='ì—°ë„', how='inner')
        
        if not merged_t.empty and len(merged_t) > 1:
            corr_row = corr_res_df[corr_res_df['ì§€ì—­'] == reg]
            if not corr_row.empty:
                r_val = corr_row['ìƒê´€ê³„ìˆ˜(r)'].values[0]
                color = 'red' if r_val > 0.7 else ('orange' if r_val > 0.4 else 'blue')
                sns.regplot(data=merged_t, x=reg, y='ì§€ì›ììˆ˜', ax=ax, color=color, scatter_kws={'alpha':0.5})
                ax.set_title(f"{reg} (r={r_val:.2f})")
            else:
                ax.text(0.5, 0.5, "No Corr", ha='center', va='center')
        else:
            ax.text(0.5, 0.5, "Data Lack", ha='center', va='center')
        ax.set_xlabel("")
        ax.set_ylabel("")

plt.tight_layout()
st.pyplot(fig_grid)

# Macro Trends: Early(Refined) vs Regular vs Total (with Forecasting)
st.markdown("##### ğŸ“ˆ ë§¤í¬ë¡œ íŠ¸ë Œë“œ: ë¶„ë¥˜êµ°ë³„ ì§€ì›ì ìˆ˜ ë° ì„±ì  ì¶”ì´ (ê³¼ê±° + 5ê°œë…„ ì˜ˆì¸¡)")
st.caption("> ì‹¤ì„ ì€ ê³¼ê±° ë°ì´í„°, ì ì„ ì€ ì¸êµ¬ ì¶”ì´ ê¸°ë°˜ 5ê°œë…„ ì˜ˆì¸¡ì¹˜ì…ë‹ˆë‹¤. ìˆ˜ì‹œ(ì¼ë°˜)ì€ ì£¼ìš” 6ê°œ ì „í˜•ì´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

# 1. Get Forecast Data for each segment
pred_e, _ = get_future_prediction_data(df[df['ë¶„ì„ê·¸ë£¹'] == 'ìˆ˜ì‹œ(ì¼ë°˜)'], df_dense_all)
pred_r, _ = get_future_prediction_data(df[df['ë¶„ì„ê·¸ë£¹'] == 'ì •ì‹œ'], df_dense_all)
pred_t, _ = get_future_prediction_data(df, df_dense_all)

# 2. Historical Data
df_e = df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹'] == 'ìˆ˜ì‹œ(ì¼ë°˜)'].groupby('í•™ë…„ë„').agg(ì§€ì›ììˆ˜=('ìˆ˜í—˜ë²ˆí˜¸', 'count'), í‰ê· ì„±ì =('ëŒ€í‘œì„±ì ', 'mean')).reset_index()
df_r = df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹'] == 'ì •ì‹œ'].groupby('í•™ë…„ë„').agg(ì§€ì›ììˆ˜=('ìˆ˜í—˜ë²ˆí˜¸', 'count'), í‰ê· ì„±ì =('ëŒ€í‘œì„±ì ', 'mean')).reset_index()
df_t = df_filtered.groupby('í•™ë…„ë„').agg(ì§€ì›ììˆ˜=('ìˆ˜í—˜ë²ˆí˜¸', 'count'), í‰ê· ì„±ì =('ëŒ€í‘œì„±ì ', 'mean')).reset_index()

nat_pop_lag = df_dense_all[['ì—°ë„', 'ì „êµ­']].copy()
nat_pop_lag['ì—°ë„_lag'] = nat_pop_lag['ì—°ë„'] + 1

# Macro Trends: Split into two subplots for clarity
st.markdown("##### ğŸ“ˆ ë§¤í¬ë¡œ íŠ¸ë Œë“œ: ì¸êµ¬ ì ˆë²½ê³¼ ì…ê²° ì˜í–¥ ë¶„ì„ (ê³¼ê±° + 5ê°œë…„ ì˜ˆì¸¡)")
st.info("""
**ğŸ“Š í†µí•© íŠ¸ë Œë“œ ì½ëŠ” ë²•**:
- **ìƒë‹¨ ì°¨íŠ¸**: ì „êµ­ ê³ 3 ì¸êµ¬ê°€ ê¸‰ê²©íˆ ê°ì†Œí•¨ì— ë”°ë¼ ìš°ë¦¬ ëŒ€í•™ ì§€ì›ì ìˆ˜(ì‹¤ì„ /ì ì„ )ê°€ ë™ë°˜ í•˜ë½í•˜ëŠ” íë¦„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
- **í•˜ë‹¨ ì°¨íŠ¸**: ìƒìœ„ê¶Œ ëŒ€í•™ì˜ 'ì¸ì› í¡ìˆ˜' íš¨ê³¼ë¡œ ì¸í•´, ìš°ë¦¬ ëŒ€í•™ì— ìœ ì…ë˜ëŠ” í•™ìƒë“¤ì˜ í‰ê·  ì„±ì (ë“±ê¸‰ ìˆ«ì)ì€ ì ì°¨ ë†’ì•„ì§ˆ(ìš°í•˜í–¥) ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤.
""")

# Prepare data for plot
fig_macro, (ax_vol, ax_grd) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)

# 1. VOLUME PLOT (Population vs Applicants)
ax_v_pop = ax_vol.twinx()
# Population (Fill Area)
ax_v_pop.fill_between(nat_pop_lag['ì—°ë„_lag'], 0, nat_pop_lag['ì „êµ­'], color='grey', alpha=0.1, label='ì „êµ­ ê³ 3 ì¸êµ¬(lag)')
ax_v_pop.set_ylabel("ì „êµ­ ì¸êµ¬ (ëª…)", color='grey')
ax_v_pop.set_ylim(0, nat_pop_lag['ì „êµ­'].max() * 1.2)

# Applicants (Lines)
sns.lineplot(data=df_t, x='í•™ë…„ë„', y='ì§€ì›ììˆ˜', ax=ax_vol, color='purple', marker='s', linewidth=2, label='ì „ì²´ ì§€ì›ì(ê³¼ê±°)')
sns.lineplot(data=df_e, x='í•™ë…„ë„', y='ì§€ì›ììˆ˜', ax=ax_vol, color='blue', marker='o', linewidth=2, label='ìˆ˜ì‹œ(ì¼ë°˜) ì§€ì›ì(ê³¼ê±°)')
sns.lineplot(data=df_r, x='í•™ë…„ë„', y='ì§€ì›ììˆ˜', ax=ax_vol, color='red', marker='^', linewidth=2, label='ì •ì‹œ ì§€ì›ì(ê³¼ê±°)')

if not pred_t.empty:
    sns.lineplot(data=pred_t, x='ì—°ë„', y='ì˜ˆì¸¡ì§€ì›ììˆ˜', ax=ax_vol, color='purple', linestyle='--', alpha=0.8)
    sns.lineplot(data=pred_e, x='ì—°ë„', y='ì˜ˆì¸¡ì§€ì›ììˆ˜', ax=ax_vol, color='blue', linestyle='--', alpha=0.8)
    sns.lineplot(data=pred_r, x='ì—°ë„', y='ì˜ˆì¸¡ì§€ì›ììˆ˜', ax=ax_vol, color='red', linestyle='--', alpha=0.8)
    
    # Connect last historical to first forecast
    l_yr = df_t['í•™ë…„ë„'].max()
    f_yr = pred_t['ì—°ë„'].min()
    for d_hist, d_pred, clr in [(df_t, pred_t, 'purple'), (df_e, pred_e, 'blue'), (df_r, pred_r, 'red')]:
        ax_vol.plot([l_yr, f_yr], [d_hist[d_hist['í•™ë…„ë„']==l_yr]['ì§€ì›ììˆ˜'].values[0], d_pred[d_pred['ì—°ë„']==f_yr]['ì˜ˆì¸¡ì§€ì›ììˆ˜'].values[0]], color=clr, linestyle='--', alpha=0.5)

ax_vol.set_ylabel("ë³¸êµ ì§€ì›ì ìˆ˜ (ëª…)")
ax_vol.set_title("ì¸êµ¬ ê°ì†Œì— ë”°ë¥¸ ì§€ì›ì ìœ ì… ê·œëª¨ ì˜ˆì¸¡", fontsize=12, pad=15)
ax_vol.legend(loc='upper right', fontsize='small')
ax_vol.grid(True, axis='y', linestyle=':', alpha=0.5)

# 2. GRADE PLOT (Quality Trend)
sns.lineplot(data=df_t, x='í•™ë…„ë„', y='í‰ê· ì„±ì ', ax=ax_grd, color='purple', marker='s', label='ì „ì²´ í‰ê· (ê³¼ê±°)')
sns.lineplot(data=df_e, x='í•™ë…„ë„', y='í‰ê· ì„±ì ', ax=ax_grd, color='blue', marker='o', label='ìˆ˜ì‹œ(ì¼ë°˜) í‰ê· (ê³¼ê±°)')
sns.lineplot(data=df_r, x='í•™ë…„ë„', y='í‰ê· ì„±ì ', ax=ax_grd, color='red', marker='^', label='ì •ì‹œ í‰ê· (ê³¼ê±°)')

if not pred_t.empty:
    sns.lineplot(data=pred_t, x='ì—°ë„', y='ì˜ˆì¸¡í‰ê· ì„±ì ', ax=ax_grd, color='purple', linestyle='--', alpha=0.8)
    sns.lineplot(data=pred_e, x='ì—°ë„', y='ì˜ˆì¸¡í‰ê· ì„±ì ', ax=ax_grd, color='blue', linestyle='--', alpha=0.8)
    sns.lineplot(data=pred_r, x='ì—°ë„', y='ì˜ˆì¸¡í‰ê· ì„±ì ', ax=ax_grd, color='red', linestyle='--', alpha=0.8)
    
    # Connect
    for d_hist, d_pred, clr in [(df_t, pred_t, 'purple'), (df_e, pred_e, 'blue'), (df_r, pred_r, 'red')]:
        ax_grd.plot([l_yr, f_yr], [d_hist[d_hist['í•™ë…„ë„']==l_yr]['í‰ê· ì„±ì '].values[0], d_pred[d_pred['ì—°ë„']==f_yr]['ì˜ˆì¸¡í‰ê· ì„±ì '].values[0]], color=clr, linestyle='--', alpha=0.5)

ax_grd.set_ylabel("í‰ê·  ì„±ì  (ë“±ê¸‰)")
ax_grd.set_title("ì§€ì› ê²½ìŸ í•˜ë½ì— ë”°ë¥¸ ìµœì¢…ë“±ë¡ì ì„±ì (Quality) ë³€í™” ì˜ˆì¸¡", fontsize=12, pad=15)
ax_grd.invert_yaxis() # 1 grade at top!
ax_grd.legend(loc='lower right', fontsize='small')
ax_grd.grid(True, axis='y', linestyle=':', alpha=0.5)

plt.tight_layout()
st.pyplot(fig_macro)
st.markdown("---")

# --- 3. Admissions Trends (ğŸ“ˆ ì…ì‹œ ì£¼ìš” ì§€í‘œ ë° íŠ¸ë Œë“œ) ---
st.header("ğŸ“ˆ 2. ì—°ë„ë³„ ì…ì‹œ ê²°ê³¼ ë° ì§€ì› íŠ¸ë Œë“œ")

# Trend Insight
st.success("""
**ğŸ’¡ ì§€ì› íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸**: 
- í‰ê·  ì„±ì ì´ ìƒìŠ¹(ìˆ«ì í•˜ê°•)í•˜ê³  ìˆë‹¤ë©´ í•´ë‹¹ ìœ í˜•ì˜ ì¸ê¸°ê°€ ë†’ì•„ì§€ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 
- ë°˜ëŒ€ë¡œ ì„±ì ì´ í•˜ë½(ìˆ«ì ìƒìŠ¹)í•˜ê³  ìˆë‹¤ë©´ ê²½ìŸ ëŒ€í•™ìœ¼ë¡œì˜ ìœ ì¶œì´ë‚˜ ì§€ì›ì í’€ì˜ ì§ˆì  ì €í•˜ë¥¼ ê²½ê³„í•˜ê³ , íƒ€ê²Ÿ í™ë³´ë¥¼ ê°•í™”í•´ì•¼ í•©ë‹ˆë‹¤.
""")

# Yearly Trend (Old Tab 1)
col_trend1, col_trend2 = st.columns(2)
# Yearly Trend (Early/Regular/Total)
col_trend1, col_trend2 = st.columns(2)
with col_trend1:
    st.subheader("ì—°ë„ë³„ í‰ê·  ì„±ì  ë¹„êµ")
    trend_e = df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹'] == 'ìˆ˜ì‹œ(ì¼ë°˜)'].groupby('í•™ë…„ë„')['ëŒ€í‘œì„±ì '].mean().reset_index()
    trend_r = df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹'] == 'ì •ì‹œ'].groupby('í•™ë…„ë„')['ëŒ€í‘œì„±ì '].mean().reset_index()
    trend_t = df_filtered.groupby('í•™ë…„ë„')['ëŒ€í‘œì„±ì '].mean().reset_index()
    
    fig1, ax1 = plt.subplots()
    ax1.plot(trend_t['í•™ë…„ë„'], trend_t['ëŒ€í‘œì„±ì '], label='ì „ì²´ í•©ê³„', marker='o', color='purple', linewidth=3)
    ax1.plot(trend_e['í•™ë…„ë„'], trend_e['ëŒ€í‘œì„±ì '], label='ìˆ˜ì‹œ(ì¼ë°˜)', marker='s', color='blue')
    ax1.plot(trend_r['í•™ë…„ë„'], trend_r['ëŒ€í‘œì„±ì '], label='ì •ì‹œ', marker='^', color='red')
    
    ax1.set_title("ë¶„ë¥˜êµ°ë³„ í‰ê·  ì„±ì  ì¶”ì´")
    ax1.invert_yaxis()
    ax1.legend()
    st.pyplot(fig1)

# Competition Trend (Old Tab 6)
# Competition Trend (Early/Regular/Total)
with col_trend2:
    st.subheader("ë¶„ë¥˜êµ°ë³„ ê²½ìŸë¥  ì¶”ì´")
    
    def get_comp_trend(target_df, label):
        agg = target_df.groupby('í•™ë…„ë„').apply(
            lambda x: pd.Series({'ì§€ì›ììˆ˜': len(x), 'í•©ê²©ììˆ˜': x['í•©ê²©ì—¬ë¶€'].sum()})
        ).reset_index()
        agg['ê²½ìŸë¥ '] = (agg['ì§€ì›ììˆ˜'] / agg['í•©ê²©ììˆ˜']).replace([np.inf, -np.inf], 0).fillna(0)
        agg['Group'] = label
        return agg

    trend_e_c = get_comp_trend(df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹'] == 'ìˆ˜ì‹œ(ì¼ë°˜)'], 'ìˆ˜ì‹œ(ì¼ë°˜)')
    trend_r_c = get_comp_trend(df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹'] == 'ì •ì‹œ'], 'ì •ì‹œ')
    trend_t_c = get_comp_trend(df_filtered, 'ì „ì²´ í•©ê³„')
    
    comp_trend_all = pd.concat([trend_e_c, trend_r_c, trend_t_c])
    
    fig6, ax6 = plt.subplots()
    sns.lineplot(data=comp_trend_all, x='í•™ë…„ë„', y='ê²½ìŸë¥ ', hue='Group', palette={'ìˆ˜ì‹œ(ì¼ë°˜)': 'blue', 'ì •ì‹œ': 'red', 'ì „ì²´ í•©ê³„': 'purple'}, marker='s', ax=ax6)
    ax6.set_title("ë¶„ë¥˜êµ°ë³„ ê²½ìŸë¥  ë³€í™”")
    st.pyplot(fig6)

# --- 4. Detailed Analysis ---
st.header("ğŸ“Š 3. ì „í˜• ë° ì„±ì  ìƒì„¸ ë¶„ì„")

# Detailed Insight
st.info("""
**ğŸ’¡ ì‹¬ì¸µ ë¶„ì„ ì¸ì‚¬ì´íŠ¸**: 
- ì „í˜•ê·¸ë£¹ë³„ ì„±ì  ë¶„í¬ì—ì„œ **ìˆ˜ì—¼(Whiskers)**ì´ ê¸´ ì „í˜•ì€ ì§€ì›ìì˜ ì„±ê²© ìŠ¤í™íŠ¸ëŸ¼ì´ ë§¤ìš° ë„“ìŒì„ ì˜ë¯¸í•˜ë©°, ì„ ë°œ ê²°ê³¼ì˜ ì˜ˆì¸¡ ê°€ëŠ¥ì„±ì´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
- **ë°”ì´ì˜¬ë¦° í”Œë¡¯**ì„ í†µí•´ ê° ì „í˜•ì˜ 'ì„±ì  ë°€ì§‘ë„'ë¥¼ í™•ì¸í•˜ì‹­ì‹œì˜¤. ë°€ë„ê°€ íŠ¹ì • êµ¬ê°„ì— ì¢ê²Œ í˜•ì„±ëœ ì „í˜•ì€ í•´ë‹¹ ì„±ì ëŒ€ì˜ í•™ìƒë“¤ì—ê²Œ ê°•ë ¥í•œ ë¸Œëœë“œ íŒŒì›Œë¥¼ ê°€ì§ì„ ëœ»í•©ë‹ˆë‹¤.
""")

col4a, col4b = st.columns(2)
with col4a:
    sel_seg_4 = st.radio("ë¶„ì„ ê·¸ë£¹ ì„ íƒ (ë°”ì´ì˜¬ë¦°)", ["ì „ì²´ í•©ê³„", "ìˆ˜ì‹œ(ì¼ë°˜)", "ì •ì‹œ"], horizontal=True)
    st.subheader(f"{sel_seg_4} ì „í˜• ê·¸ë£¹ë³„ ì„±ì  ë¶„í¬")
    df_v4 = df_filtered if sel_seg_4 == "ì „ì²´ í•©ê³„" else df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹'] == sel_seg_4]
    if not df_v4.empty:
        fig2a, ax2a = plt.subplots(figsize=(8, 5))
        sns.violinplot(data=df_v4, x='ì „í˜•ê·¸ë£¹', y='ëŒ€í‘œì„±ì ', ax=ax2a, palette='Set3')
        plt.xticks(rotation=45)
        ax2a.invert_yaxis()
        st.pyplot(fig2a)

with col4b:
    st.subheader("í•©ê²©/ë“±ë¡ ì„±ì  ë¶„í¬ (Funnel)")
    target_pr = st.radio("ëŒ€ìƒ ì„ íƒ", ["í•©ê³„(ì „ì²´)", "ìˆ˜ì‹œ(ì¼ë°˜)", "ì •ì‹œ"], key='pr_v', horizontal=True)
    if target_pr == "í•©ê³„(ì „ì²´)":
        df_pr = df_filtered.dropna(subset=['ëŒ€í‘œì„±ì '])
    else:
        df_pr = df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹']==target_pr].dropna(subset=['ëŒ€í‘œì„±ì '])
        
    if not df_pr.empty:
        fig5, ax5 = plt.subplots(figsize=(6, 4))
        sns.kdeplot(df_pr['ëŒ€í‘œì„±ì '], label='ì „ì²´', fill=True, ax=ax5)
        sns.kdeplot(df_pr[df_pr['í•©ê²©êµ¬ë¶„']=='ìµœì´ˆí•©ê²©']['ëŒ€í‘œì„±ì '], label='ìµœì´ˆí•©ê²©', fill=True, ax=ax5)
        sns.kdeplot(df_pr[df_pr['ë“±ë¡êµ¬ë¶„']=='ë“±ë¡']['ëŒ€í‘œì„±ì '], label='ë“±ë¡', fill=True, ax=ax5)
        plt.legend()
        st.pyplot(fig5)

with st.expander("ğŸŒ ì§€ì—­ë³„ x ì „í˜•ê·¸ë£¹ ìƒì„¸ ë¶„ì„ (Heatmap)"):
    df_region = df_filtered[df_filtered['ì…í•™ìœ í˜•'] == 'ìˆ˜ì‹œ']
    if not df_region.empty:
        top_regions = df_region['êµìœ¡ì²­ì†Œì¬ì§€'].value_counts().nlargest(15).index
        region_pivot = df_region[df_region['êµìœ¡ì²­ì†Œì¬ì§€'].isin(top_regions)].pivot_table(
            index='êµìœ¡ì²­ì†Œì¬ì§€', columns='ì „í˜•ê·¸ë£¹', values='ëŒ€í‘œì„±ì ', aggfunc='mean'
        )
        fig_heat, ax_heat = plt.subplots(figsize=(10, 6))
        sns.heatmap(region_pivot, annot=True, fmt=".2f", cmap="YlGnBu", ax=ax_heat)
        st.pyplot(fig_heat)
# --- 5. Regional Insights ---
st.header("ğŸ¯ 4. ì§€ì—­ë³„ íƒ€ê²ŸíŒ… ì „ëµ (Regional Insights)")

# Regional Insight
st.success("""
**ğŸ’¡ ì§€ì—­ íƒ€ê²ŸíŒ… ì¸ì‚¬ì´íŠ¸**: 
- íŠ¹ì • ì§€ì—­ì˜ **ìµœì´ˆë“±ë¡ë¥ (Yield)**ì´ ë‚®ë‹¤ë©´, í•´ë‹¹ ì§€ì—­ì˜ í•©ê²©ìë“¤ì´ íƒ€ ëŒ€í•™ìœ¼ë¡œ ì´íƒˆí•˜ê³  ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. íŠ¹íˆ ì§€ì› ë¹„ì¤‘ì´ ë†’ì€ ì§€ì—­ì—ì„œì˜ ì´íƒˆì€ ë¼ˆì•„í”ˆ íƒ€ê²©ì…ë‹ˆë‹¤. 
- ìš¸ì‚° ë“± ì„±ì¥ì´ ëšœë ·í•œ ì§€ì—­ì— ëŒ€í•´ì„œëŠ” ë©´ì ‘ ë°°ì • í™•ëŒ€ë‚˜ ê³ êµ ë°©ë¬¸ ì„¤ëª…íšŒ ë“± ì§‘ì¤‘ì ì¸ ìì› íˆ¬ì…ì´ í•„ìš”í•©ë‹ˆë‹¤.
""")

# Regional Insights with Segment Selection
st.header("ğŸ¯ 4. ì§€ì—­ë³„ íƒ€ê²ŸíŒ… ì „ëµ (Regional Insights)")

sel_reg_seg = st.radio("ì§€ì—­ ë¶„ì„ ëŒ€ìƒ ì„ íƒ", ["ì „ì²´ í•©ê³„", "ìˆ˜ì‹œ(ì¼ë°˜)", "ì •ì‹œ"], key='reg_seg_sel', horizontal=True)
df_reg_target = df_filtered if sel_reg_seg == "ì „ì²´ í•©ê³„" else df_filtered[df_filtered['ë¶„ì„ê·¸ë£¹'] == sel_reg_seg]

reg_table = df_reg_target.groupby('êµìœ¡ì²­ì†Œì¬ì§€').agg(
    ì§€ì›ììˆ˜=('ìˆ˜í—˜ë²ˆí˜¸', 'count'),
    ìµœì´ˆí•©ê²©ììˆ˜=('í•©ê²©êµ¬ë¶„', lambda x: x.astype(str).str.contains('ìµœì´ˆ').sum()),
    ë“±ë¡ììˆ˜=('ë“±ë¡êµ¬ë¶„', lambda x: (x=='ë“±ë¡').sum()),
    í‰ê· ë“±ê¸‰=('ëŒ€í‘œì„±ì ', 'mean')
).sort_values('ì§€ì›ììˆ˜', ascending=False)

reg_table['ë“±ë¡ë¥ (%)'] = (reg_table['ë“±ë¡ììˆ˜'] / reg_table['ì§€ì›ììˆ˜'] * 100).fillna(0)
reg_table['ìµœì´ˆë“±ë¡ë¥ (%)'] = (reg_table['ë“±ë¡ììˆ˜'] / reg_table['ìµœì´ˆí•©ê²©ììˆ˜'] * 100).replace([np.inf, -np.inf], 0).fillna(0)
reg_table['ë¹„ì¤‘(%)'] = (reg_table['ì§€ì›ììˆ˜'] / reg_table['ì§€ì›ììˆ˜'].sum() * 100)

col_reg1, col_reg2 = st.columns([1, 1])
with col_reg1:
    st.markdown("##### ğŸ“Š ì§€ì—­ë³„ ì§€ì› í˜„í™© (Top 20)")
    st.dataframe(reg_table.head(20).style.format("{:.2f}", subset=['í‰ê· ë“±ê¸‰', 'ë“±ë¡ë¥ (%)', 'ìµœì´ˆë“±ë¡ë¥ (%)', 'ë¹„ì¤‘(%)']))

with col_reg2:
    st.markdown("##### ğŸ¯ ìš¸ì‚° ì§€ì—­ ì‹¬ì¸µ ë¶„ì„")
    if 'ìš¸ì‚°' in reg_table.index:
        ulsan_stats = reg_table.loc['ìš¸ì‚°']
        last_year = df_filtered['í•™ë…„ë„'].max()
        share, grade, yield_rate_n = ulsan_stats['ë¹„ì¤‘(%)'], ulsan_stats['í‰ê· ë“±ê¸‰'], ulsan_stats['ìµœì´ˆë“±ë¡ë¥ (%)']
        
        c1, c2 = st.columns(2)
        with c1: st.metric("ìš¸ì‚° ì§€ì› ë¹„ì¤‘", f"{share:.1f}%")
        with c2: st.metric("ìš¸ì‚° í‰ê·  ë“±ê¸‰", f"{grade:.2f} ë“±ê¸‰")
        
        try:
            ulsan_curr = df_filtered[(df_filtered['í•™ë…„ë„'] == last_year) & (df_filtered['êµìœ¡ì²­ì†Œì¬ì§€'] == 'ìš¸ì‚°')].shape[0]
            ulsan_prev = df_filtered[(df_filtered['í•™ë…„ë„'] == last_year - 1) & (df_filtered['êµìœ¡ì²­ì†Œì¬ì§€'] == 'ìš¸ì‚°')].shape[0]
            if ulsan_curr > ulsan_prev:
                st.success(f"ğŸ“ˆ {last_year}í•™ë…„ë„ ìš¸ì‚° ì§€ì›ì ê¸‰ì¦ (+{ulsan_curr-ulsan_prev}ëª…)")
        except: pass

        st.metric("ğŸ’¡ ìµœì´ˆí•©ê²©ì ë“±ë¡ë¥  (Initial Yield)", f"{yield_rate_n:.1f}%", delta=f"ì´íƒˆë¥ : {100-yield_rate_n:.1f}%", delta_color="inverse")
        st.progress(min(int(yield_rate_n), 100))
        
        if yield_rate_n < 50: st.error("âš ï¸ **ìš°ìˆ˜ ìì› ìœ ì¶œ ì‹¬ê°**: ë¦¬í…ì…˜ ì „ëµ ì‹œê¸‰")
        else: st.success("âœ… **ì•ˆì •ì  ì¶©ì„±ë„ ìœ ì§€**")

st.markdown("---")

# --- 6. Efficiency Analysis ---
st.header("ğŸ—£ï¸ 5. ì „í˜• ìš´ì˜ íš¨ìœ¨í™” ë° ë©´ì ‘ ì˜í–¥ë ¥")

# Efficiency Insight
st.success("""
**ğŸ’¡ íš¨ìœ¨í™” ì¸ì‚¬ì´íŠ¸**: 
- **ë°”ì´ì˜¬ë¦° í”Œë¡¯(Violin Plot)**ì€ ì„±ì  ë¶„í¬ì˜ ë°€ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë©´ì ‘ ì „í˜•ì´ êµê³¼ ì„±ì  í•˜ìœ„ êµ¬ê°„ê¹Œì§€ ë„“ê²Œ í¼ì ¸ ìˆë‹¤ë©´, ì´ëŠ” ë©´ì ‘ì´ ì‹¤ì§ˆì ì¸ ì—­ì „ì˜ ê¸°íšŒë¥¼ ì œê³µí•˜ê³  ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. 
- ë©´ì ‘ì˜ ì„±ì  ë³´ì • íš¨ê³¼ê°€ í´ìˆ˜ë¡, ë‹¨ìˆœ êµê³¼ ì„±ì  ìœ„ì£¼ ì„ ë°œì—ì„œ ë²—ì–´ë‚œ ë‹¤ë©´ì  í‰ê°€ê°€ ì´ë£¨ì–´ì§€ê³  ìˆë‹¤ëŠ” ì¦ê±°ì…ë‹ˆë‹¤.
""")

df_susi = df_filtered[df_filtered['ì…í•™ìœ í˜•'] == 'ìˆ˜ì‹œ'].copy()
def get_paren_tag(text):
    import re
    matches = re.findall(r'\(([^)]+)\)', str(text))
    return matches[-1] if matches else "ì¼ë°˜"

df_susi['ì„¸ë¶€ìœ í˜•'] = df_susi['ì „í˜•êµ¬ë¶„'].apply(get_paren_tag)
stats_by_tag = df_susi.groupby('ì„¸ë¶€ìœ í˜•')['ëŒ€í‘œì„±ì '].agg(['count', 'mean'])

c_eff1, c_eff2 = st.columns([1, 1])
with c_eff1:
    st.markdown("##### ë©´ì ‘ ìœ ë¬´ë³„ ì„±ì  ë¶„í¬ (Violin Plot)")
    fig4, ax4 = plt.subplots(figsize=(6, 4))
    sns.violinplot(data=df_susi, x='ë©´ì ‘ìœ ë¬´', y='ëŒ€í‘œì„±ì ', palette='Pastel1', split=True, ax=ax4)
    ax4.invert_yaxis()
    st.pyplot(fig4)

with c_eff2:
    interview_tags = [t for t in stats_by_tag.index if 'ë©´ì ‘' in t]
    non_interview_tags = [t for t in stats_by_tag.index if 'ë©´ì ‘' not in t]
    if interview_tags and non_interview_tags:
        avg_int, avg_non = stats_by_tag.loc[interview_tags, 'mean'].mean(), stats_by_tag.loc[non_interview_tags, 'mean'].mean()
        if pd.notna(avg_int) and pd.notna(avg_non):
            st.info(f"ğŸ’¡ **ë©´ì ‘ì˜ ì„±ì  ë³´ì • íš¨ê³¼**: ë©´ì ‘ ì „í˜•ì€ êµê³¼ ì „í˜• ëŒ€ë¹„ í‰ê·  **{avg_non - avg_int:.2f}ë“±ê¸‰** ë‚®ì€ í•™ìƒë“¤ì—ê²Œ í•©ê²© ê¸°íšŒë¥¼ ì œê³µí•˜ë©° ê³µì •ì„±ì„ ë³´ì™„í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

st.markdown("---")
# --- 6. Quota Strategy Simulation (ğŸ—ï¸ ì „í˜•ë³„ ì •ì› ì¡°ì • ì‹œë®¬ë ˆì´ì…˜) ---
st.header("ğŸ—ï¸ 6. ìˆ˜ì‹œ(ì¼ë°˜) ì „í˜•ë³„ ì •ì› ì¡°ì • ì‹œë®¬ë ˆì´ì…˜")
st.markdown("> **ëŒ€ìƒ**: ìˆ˜ì‹œ(ì¼ë°˜) - ìˆ˜ëŠ¥ìœ„ì£¼, ê¸°íšŒê· ë“±, ë†ì–´ì´Œ, ê¸°ì´ˆìƒí™œ, ì¬ì™¸êµ­ë¯¼, íŠ¹ì„±í™”ê³ êµ ì „í˜• ì œì™¸")

# Filter for Refined Early Admissions only
df_sim = df_filtered[(df_filtered['í•™ë…„ë„'] == last_year) & (df_filtered['ë¶„ì„ê·¸ë£¹'] == 'ìˆ˜ì‹œ(ì¼ë°˜)')]

if not df_sim.empty:
    # Aggregate by Admission Group (ì „í˜•) instead of Department
    jeon_stats = df_sim.groupby('ì „í˜•ê·¸ë£¹').agg(
        ì§€ì›ììˆ˜=('ìˆ˜í—˜ë²ˆí˜¸', 'count'),
        í•©ê²©ììˆ˜=('í•©ê²©ì—¬ë¶€', 'sum'),
        ë“±ë¡ìí‰ê· ì„±ì =('ëŒ€í‘œì„±ì ', lambda x: x[df_sim.loc[x.index, 'ë“±ë¡êµ¬ë¶„'] == 'ë“±ë¡'].mean())
    ).reset_index()
    jeon_stats['ê²½ìŸë¥ '] = (jeon_stats['ì§€ì›ììˆ˜'] / jeon_stats['í•©ê²©ììˆ˜']).fillna(0)
    
    med_comp_j, med_grade_j = jeon_stats['ê²½ìŸë¥ '].median(), jeon_stats['ë“±ë¡ìí‰ê· ì„±ì '].median()
    
    def get_matrix_type_jeon(row):
        is_high_comp = row['ê²½ìŸë¥ '] >= med_comp_j
        is_good_grade = row['ë“±ë¡ìí‰ê· ì„±ì '] <= med_grade_j # Lower is better
        if is_high_comp and is_good_grade: return "Star"
        elif is_high_comp and not is_good_grade: return "Cash Cow"
        elif not is_high_comp and is_good_grade: return "Hidden Gem"
        else: return "Dog"
    
    jeon_stats['Type'] = jeon_stats.apply(get_matrix_type_jeon, axis=1)
    
    col_sim_cfg1, col_sim_cfg2 = st.columns([1, 2])
    with col_sim_cfg1:
        scenario = st.radio("ì‹œë®¬ë ˆì´ì…˜ ê°•ë„", ('ë³´ìˆ˜ì (10%)', 'ì ê·¹ì (20%)', 'ê¸‰ì§„ì (30%)' ), index=1, horizontal=True)
        cut_rate = 0.1 if 'ë³´ìˆ˜ì ' in scenario else (0.2 if 'ì ê·¹ì ' in scenario else 0.3)
    
    with col_sim_cfg2:
        st.info(f"ì„ íƒí•œ ì‹œë‚˜ë¦¬ì˜¤ì— ë”°ë¼ **ê²½ìŸë¥ ê³¼ ì„±ì ì´ ëª¨ë‘ ë‚®ì€ ì „í˜•(Dog)**ì˜ ì •ì›ì„ ì°¨ì¶œí•˜ì—¬ **ìš°ìˆ˜ ì „í˜•(Star)**ìœ¼ë¡œ ì¬ë°°ì¹˜í•©ë‹ˆë‹¤.")

    # --- ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ ë¡œì§ ì„¤ì • ---
    # 1. ì œë¡œì„¬(Zero-Sum) ì›ì¹™ ì ìš©: ì „ì²´ ì…ì‹œ ì •ì›ì€ ìœ ì§€í•œ ì±„, ë‚´ë¶€ ë¹„ì¤‘ë§Œ ì¡°ì •í•˜ì—¬ ëŒ€í•™ì˜ ìš´ì˜ ë¶€ë‹´ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.
    # 2. Dog(ì €ì¡°) ì „í˜• ê°ì¶•: ì„±ì ê³¼ ê²½ìŸë¥ ì´ ëª¨ë‘ ë‚®ì€ ì „í˜•ì€ 'ì„ ì •ì  ê°€ì¹˜'ê°€ ë‚®ë‹¤ê³  íŒë‹¨í•˜ì—¬ ìš°ì„  ê°ì¶• ëŒ€ìƒìœ¼ë¡œ ì„ ì •í•©ë‹ˆë‹¤.
    # 3. ìš°ìˆ˜ ì „í˜• ì¦ì›: ì„±ì ì´ ë†’ê³  ê²½ìŸë¥ ì´ ì¹˜ì—´í•œ 'Star' ì „í˜•ì— ì •ì›ì„ ëª°ì•„ì£¼ì–´ ì „ì²´ì ì¸ í•©ê²©ì í’ˆì§ˆì„ ìƒë°© í‰ì¤€í™”í•©ë‹ˆë‹¤.
    
    total_pool = 0 # ê°ì¶•ëœ ì´ ì¸ì›ì„ ë‹´ëŠ” ë°”êµ¬ë‹ˆ
    jeon_stats['Adj'] = 0 # ê° ì „í˜•ë³„ ì¡°ì • ì¸ì›
    
    # [ê³„ì‚°] Dog ì „í˜•ì—ì„œ ì •ì›ì„ ëºì–´ì˜¤ê¸°
    for idx, row in jeon_stats.iterrows():
        if row['Type'] == 'Dog':
            cut = int(row['í•©ê²©ììˆ˜'] * cut_rate)
            jeon_stats.at[idx, 'Adj'] = -cut
            total_pool += cut # ëºì–´ì˜¨ ì¸ì›ì„ ë°”êµ¬ë‹ˆì— í•©ì‚°
    
    # [ê³„ì‚°] ë°”êµ¬ë‹ˆì— ë‹´ê¸´ ì¸ì›ì„ ìš°ìˆ˜ ì „í˜•ì— ê³¨ê³ ë£¨ ë‚˜ëˆ ì£¼ê¸° (Star -> Hidden Gem ìˆœì„œ)
    if total_pool > 0:
        targets = jeon_stats[jeon_stats['Type'] == 'Star'] # 1ìˆœìœ„: ì„±ì +ê²½ìŸë¥  ëª¨ë‘ ì¢‹ì€ ì „í˜•
        if targets.empty:
            targets = jeon_stats[jeon_stats['Type'] == 'Hidden Gem'] # 2ìˆœìœ„: ì„±ì ì€ ì¢‹ì€ë° í™ë³´ê°€ í•„ìš”í•œ ì „í˜•
        if targets.empty:
            targets = jeon_stats[jeon_stats['Type'] == 'Cash Cow'] # 3ìˆœìœ„: ì„±ì ì€ ë‚®ì•„ë„ ì¸ê¸°ëŠ” ìˆëŠ” ì „í˜•
            
        if not targets.empty:
            per_target = total_pool // len(targets) # ê³µí‰í•˜ê²Œ ë°°ë¶„
            remainder = total_pool % len(targets) # ë‚˜ëˆ„ê³  ë‚¨ì€ ì‰ì—¬ ì¸ì›
            
            for i, idx in enumerate(targets.index):
                jeon_stats.at[idx, 'Adj'] += per_target
                if i == 0: # ì œë¡œì„¬ì„ ì™„ë²½íˆ ë§ì¶”ê¸° ìœ„í•´ ë‚¨ì€ ì°Œêº¼ê¸° ì¸ì›ì„ ì²« ë²ˆì§¸ íƒ€ê²Ÿì— í•©ì‚°
                    jeon_stats.at[idx, 'Adj'] += remainder
        else:
            jeon_stats['Adj'] = 0 # ë§Œì•½ ì¤„ ë°ê°€ ì—†ë‹¤ë©´ ì‹œë®¬ë ˆì´ì…˜ ì·¨ì†Œ
            total_pool = 0

    cur_reg_students = df_sim[df_sim['ë“±ë¡êµ¬ë¶„'] == 'ë“±ë¡'].copy()
    sim_reg_pool = cur_reg_students.copy()
    
    # Track actions for basis explanation
    action_log = []
    
    # 1. Processing Cuts (Dog Pathways)
    for idx, row in jeon_stats.iterrows():
        if row['Adj'] < 0:
            num_cut = abs(int(row['Adj']))
            # Find the worst (highest number) registered students in this group
            grp_reg = sim_reg_pool[sim_reg_pool['ì „í˜•ê·¸ë£¹'] == row['ì „í˜•ê·¸ë£¹']].sort_values('ëŒ€í‘œì„±ì ', ascending=False)
            cut_ids = grp_reg.head(num_cut).index
            sim_reg_pool = sim_reg_pool.drop(cut_ids)
            action_log.append(f"â€¢ **{row['ì „í˜•ê·¸ë£¹']}** (Dog): ì •ì› {num_cut}ëª… ì£¼ì¶• - í•´ë‹¹ ì „í˜• ë‚´ í•˜ìœ„ ì„±ì  ë“±ë¡ì {num_cut}ëª… ì œì™¸")

    # 2. Processing Gains (Target Pathways)
    for idx, row in jeon_stats.iterrows():
        if row['Adj'] > 0:
            num_add = int(row['Adj'])
            # Find the best applicants who are NOT currently registered in this target group
            # These could be 'ë¶ˆí•©ê²©' or 'ìµœì´ˆí•©ê²©(ë¯¸ë“±ë¡)' etc.
            potential_pool = df_sim[(df_sim['ì „í˜•ê·¸ë£¹'] == row['ì „í˜•ê·¸ë£¹']) & (df_sim['ë“±ë¡êµ¬ë¶„'] != 'ë“±ë¡')].sort_values('ëŒ€í‘œì„±ì ', ascending=True)
            
            if not potential_pool.empty:
                added_students = potential_pool.head(num_add)
                sim_reg_pool = pd.concat([sim_reg_pool, added_students])
                action_log.append(f"â€¢ **{row['ì „í˜•ê·¸ë£¹']}** (ìš°ìˆ˜): ì •ì› {len(added_students)}ëª… ì¦ì› - í•´ë‹¹ ì „í˜• ë¯¸ë“±ë¡ ì§€ì›ì ì¤‘ ìƒìœ„ ì„±ì  {len(added_students)}ëª… ì‹ ê·œ ìœ ì…")
            else:
                action_log.append(f"â€¢ **{row['ì „í˜•ê·¸ë£¹']}** (ìš°ìˆ˜): ì¦ì› ëŒ€ìƒì´ì—ˆìœ¼ë‚˜ í™œìš© ê°€ëŠ¥í•œ ì˜ˆë¹„ ìì›(ë¯¸ë“±ë¡ ì§€ì›ì)ì´ ë¶€ì¡±í•˜ì—¬ ì¦ì›ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # Redistribution Breakdown
    st.markdown("##### ğŸ“‹ ì „í˜•ë³„ ì •ì› ì¬ë°°ì¹˜ ìƒì„¸ ê³„íš (From -> To)")
    reductions = jeon_stats[jeon_stats['Adj'] < 0][['ì „í˜•ê·¸ë£¹', 'Adj']].rename(columns={'Adj': 'ê°ì¶•ì¸ì› (ëª…)'}).sort_values('ê°ì¶•ì¸ì› (ëª…)')
    expansions = jeon_stats[jeon_stats['Adj'] > 0][['ì „í˜•ê·¸ë£¹', 'Adj']].rename(columns={'Adj': 'ì¦ì›ì¸ì› (ëª…)'}).sort_values('ì¦ì›ì¸ì› (ëª…)', ascending=False)
    
    # Double check zero-sum
    checksum = len(sim_reg_pool) - len(cur_reg_students)
    
    col_plan1, col_plan2 = st.columns(2)
    with col_plan1:
        st.markdown(f"**ğŸ“‰ ê°ì¶• ëŒ€ìƒ (Dog ì „í˜•, ì´ {abs(reductions['ê°ì¶•ì¸ì› (ëª…)'].sum())}ëª…)**")
        st.table(reductions)
    with col_plan2:
        st.markdown(f"**ğŸ“ˆ ì¦ì› ëŒ€ìƒ (ìš°ìˆ˜ ì „í˜•, ì´ {expansions['ì¦ì›ì¸ì› (ëª…)'].sum()}ëª…)**")
        st.table(expansions)
    
    # Calculation
    cur_avg_val = cur_reg_students['ëŒ€í‘œì„±ì '].mean()
    fut_avg_val = sim_reg_pool['ëŒ€í‘œì„±ì '].mean()
    diff_val = cur_avg_val - fut_avg_val
    
    if checksum == 0 and total_pool > 0:
        st.success(f"âœ… **ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼**: ê°œë³„ ë°ì´í„°(Row-level) ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼, ì „í˜• ì¬ë°°ì¹˜ ì‹œ ì‹ ì…ìƒ í‰ê·  ì„±ì ì´ ì•½ **{diff_val:.4f} ë“±ê¸‰ í–¥ìƒ**ë  ê²ƒìœ¼ë¡œ ì˜ˆì¸¡ë©ë‹ˆë‹¤.")
        
        with st.expander("ğŸ” ì™œ ì´ë ‡ê²Œ ê³„ì‚°ë˜ì—ˆë‚˜ìš”? (ì‹œë®¬ë ˆì´ì…˜ ë¡œì§ ê·¼ê±° ìƒì„¸)"):
            st.markdown(f"""
            ì´ ì‹œë®¬ë ˆì´ì…˜ì€ ë‹¨ìˆœíˆ í‰ê·  ì ìˆ˜ë¥¼ ë”í•˜ê³  ë¹¼ëŠ” ë°©ì‹ì´ ì•„ë‹ˆë¼, **{len(df_sim):,}ëª…ì˜ ê°œë³„ ì§€ì›ìì˜ ì‹¤ì œ ì„±ì **ì„ ê°€ì§€ê³  'ë§Œì•½ ì •ì›ì´ ì´ë¬ë‹¤ë©´?'ì„ ê°€ì •í•˜ì—¬ í•˜ë‚˜í•˜ë‚˜ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤.
            
            **1) ì™œ ì„±ì  í•˜ìœ„ìë¥¼ ë¹¼ë‚˜ìš”? (ê°ì¶• ë¡œì§)**
            - ëŒ€í•™ ì…ì‹œì—ì„œ ì •ì›ì„ ì¤„ì´ë©´, ê²½ìŸì´ ì¹˜ì—´í•´ì§€ë©´ì„œ ì„±ì ì´ ê°€ì¥ ë‚®ì€ í•™ìƒë“¤ë¶€í„° í•©ê²©ê¶Œì—ì„œ ë©€ì–´ì§€ê²Œ ë©ë‹ˆë‹¤. 
            - ë”°ë¼ì„œ Dog(ì €ì¡°) ì „í˜•ì—ì„œ ì •ì›ì„ ì¤„ì¼ ë•Œ, **ì‹¤ì œ ë“±ë¡í•œ í•™ìƒ ì¤‘ ê°€ì¥ ì„±ì ì´ ë‚®ì€ í•˜ìœ„ Nëª…**ì„ ê°€ìƒìœ¼ë¡œ íƒˆë½ì‹œì¼œ ì»¤íŠ¸ë¼ì¸ ìƒìŠ¹ íš¨ê³¼ë¥¼ ì‹œë®¬ë ˆì´ì…˜í–ˆìŠµë‹ˆë‹¤.
            
            **2) ì™œ ë¯¸ë“±ë¡ ì§€ì›ìë¥¼ ë„£ë‚˜ìš”? (ì¦ì› ë¡œì§)**
            - ìš°ìˆ˜ ì „í˜•ì˜ ì •ì›ì„ ëŠ˜ë¦¬ë©´, ê¸°ì¡´ì—ëŠ” ì•„ì‰½ê²Œ ë–¨ì–´ì¡Œê±°ë‚˜ ì ìˆ˜ëŠ” ì¶©ë¶„í•œë° ë‹¤ë¥¸ ëŒ€í•™ìœ¼ë¡œ ë¹ ì ¸ë‚˜ê°„ 'ì ì¬ì  ìš°ìˆ˜ ìì›'ì´ í•©ê²©ê¶Œì— ë“¤ì–´ì˜¤ê²Œ ë©ë‹ˆë‹¤.
            - ë”°ë¼ì„œ Star(ìš°ìˆ˜) ì „í˜•ì—ì„œ ì •ì›ì„ ëŠ˜ë¦´ ë•Œ, **í˜„ì¬ ë“±ë¡í•˜ì§€ ì•Šì€ ì§€ì›ì ì¤‘ ì„±ì ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ìƒìœ„ Nëª…**ì„ ì‹ ê·œ ìœ ì…ì‹œì¼œ ì…ê²° ìƒìŠ¹ì„ ì‹œë®¬ë ˆì´ì…˜í–ˆìŠµë‹ˆë‹¤.
            
            **3) ì™œ ì œë¡œì„¬(Zero-Sum)ì¸ê°€ìš”?**
            - ëŒ€í•™ ì „ì²´ ì •ì›ì€ êµìœ¡ë¶€ ì¸ê°€ì— ë”°ë¼ ê³ ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í•œ ê³³ì„ ëŠ˜ë¦¬ë ¤ë©´ ë°˜ë“œì‹œ ë‹¤ë¥¸ ê³³ì„ ì¤„ì—¬ì•¼ í•˜ëŠ” **'í’ì„  íš¨ê³¼'**ë¥¼ ë°˜ì˜í•˜ì—¬ í˜„ì‹¤ì ì¸ ì¬ë°°ì¹˜ ì•ˆì„ ë„ì¶œí•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.
            
            **ğŸš€ ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì‹¤í–‰ ë¡œê·¸:**
            """)
            for log in action_log:
                st.write(log)
    elif total_pool == 0:
        st.info("ğŸ’¡ ì„±ì  ë° ê²½ìŸë¥ ì´ ë‚®ì€ 'Dog' ì „í˜•ì´ ì—†ê±°ë‚˜ ì¬ë°°ì¹˜í•  ëŒ€ìƒ ì „í˜•ì´ ì—†ì–´ ë³€ë™ ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning(f"âš ï¸ ì‹œë®¬ë ˆì´ì…˜ ê³„ì‚° ì¤‘ ì˜¤ì°¨ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ (ì”ì—¬: {checksum}ëª…).")
else:
    st.warning("ìˆ˜ì‹œ(ì¼ë°˜) ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

st.markdown("---")

# --- 7. Future Outlook (ğŸ”® ìƒí–¥ì‹ ì…ì‹œ ê·œëª¨ ë° ì„±ì  ì˜ˆì¸¡) ---
st.header("ğŸ”® 7. ë°ì´í„° ê¸°ë°˜ 5ê°œë…„ ì…ì‹œ ì˜ˆì¸¡ (2026~2030)")

# Forecasting Insight
st.info("""
**ğŸ’¡ ê³ ë„í™”ëœ ì˜ˆì¸¡ ëª¨ë¸**: 
- **ì¸êµ¬ ì§€ì—° íš¨ê³¼ ë°˜ì˜**: ê³ 3 í•™ë ¹ì¸êµ¬ëŠ” ê·¸ë‹¤ìŒ í•´ ì…ì‹œì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤ (Pop[N-1] -> Adm[N]). ê³ êµ ì¡¸ì—… ì˜ˆì •ìê°€ ì‹¤ì œ ìˆ˜í—˜ìƒì´ ë˜ëŠ” ì‹œì°¨ë¥¼ ë°˜ì˜í–ˆìŠµë‹ˆë‹¤.
- **ì§€ì—­ ê°€ì¤‘ì¹˜ & ìµœì‹ í™”**: ì£¼ìš” íƒ€ê²Ÿ ì§€ì—­ ì¸êµ¬ ì¶”ì´ì— ê°€ì¤‘ì¹˜ë¥¼ ë‘ê³ , ìµœê·¼ 3ê°œë…„ íŠ¸ë Œë“œë¥¼ ì§‘ì¤‘ ë°˜ì˜í•˜ì—¬ í˜„ì‹¤ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.
- **ì„±ì  ì¶• ì—­ì „**: ëª¨ë“  ì„±ì  ì°¨íŠ¸ëŠ” 1ë“±ê¸‰ì´ ìƒë‹¨ì— ìœ„ì¹˜í•˜ë„ë¡ ì¼ê´€ë˜ê²Œ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤ (1ë“±ê¸‰ = ìµœê³  ì„±ì ).
""")

# Forecasting Target Selection
sel_pred_grp = st.radio("ì˜ˆì¸¡ ëŒ€ìƒ ë¶„ë¥˜ ì„ íƒ", ["ì „ì²´ í•©ê³„", "ìˆ˜ì‹œ(ì¼ë°˜ - ì£¼ìš” 6ê°œ ì „í˜• ì œì™¸)", "ì •ì‹œ"], key='pred_grp_sel', horizontal=True)
pred_source = df if sel_pred_grp == "ì „ì²´ í•©ê³„" else df[df['ë¶„ì„ê·¸ë£¹'] == ('ì •ì‹œ' if sel_pred_grp == 'ì •ì‹œ' else 'ìˆ˜ì‹œ(ì¼ë°˜)')]

pred_df, diag_df = get_future_prediction_data(pred_source, df_dense_all)

# Dynamic Warning based on selection
grp_name = "ë³¸êµ ì „ì²´" if sel_pred_grp == "ì „ì²´ í•©ê³„" else ("ì •ì‹œ" if sel_pred_grp == "ì •ì‹œ" else "ìˆ˜ì‹œ(ì¼ë°˜)")

if not pred_df.empty:
    col_diag, col_pred = st.columns([1, 2])
    
    with col_diag:
        st.markdown(f"##### ğŸ” {grp_name} ì˜ˆì¸¡ ì§„ë‹¨")
        fig_diag, ax_d = plt.subplots(figsize=(6, 5))
        sns.regplot(data=diag_df, x='weighted_pop', y='ì§€ì›ììˆ˜', ax=ax_d, color='purple', scatter_kws={'s':100, 'alpha':0.6})
        ax_d.set_title(f"{grp_name}: ì¸êµ¬(Y-1) vs ì‹¤ì§€ì›ì(Y)")
        ax_d.set_xlabel("ì „ë…„ë„ ê°€ì¤‘ ì§€ì—­ ì¸êµ¬")
        ax_d.set_ylabel("ë‹¹í•´ ì—°ë„ ì§€ì›ì ìˆ˜")
        st.pyplot(fig_diag)
        st.caption("ì „ë…„ë„ ê³ 3 ì¸êµ¬ ë³€í™”ê°€ ê·¸ë‹¤ìŒ í•´ ì§€ì›ì ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì§ì ‘ì  ì˜í–¥ì…ë‹ˆë‹¤.")

    with col_pred:
        st.markdown("##### ğŸ“ˆ í–¥í›„ 5ê°œë…„ ê²½ìŸë¥  ë° ì„±ì  ë³€í™” ì˜ˆì¸¡")
        fig_pred, ax_p1 = plt.subplots(figsize=(10, 5))
        ax_p2 = ax_p1.twinx()
        
        # Historical stats
        sns.lineplot(data=diag_df, x='í•™ë…„ë„', y='ê²½ìŸë¥ ', ax=ax_p1, color='blue', marker='o', linewidth=2, label='ê³¼ê±° ê²½ìŸë¥ ')
        sns.lineplot(data=diag_df, x='í•™ë…„ë„', y='í‰ê· ì„±ì ', ax=ax_p2, color='red', marker='s', linewidth=2, label='ê³¼ê±° í‰ê· ì„±ì ')
        
        # Predicted stats
        sns.lineplot(data=pred_df, x='ì—°ë„', y='ì˜ˆì¸¡ê²½ìŸë¥ ', ax=ax_p1, color='blue', linestyle='--', marker='o', alpha=0.7, label='ì˜ˆì¸¡ ê²½ìŸë¥ ')
        sns.lineplot(data=pred_df, x='ì—°ë„', y='ì˜ˆì¸¡í‰ê· ì„±ì ', ax=ax_p2, color='red', linestyle='--', marker='s', alpha=0.7, label='ì˜ˆì¸¡ í‰ê· ì„±ì ')
        
        # Connect last actual to first predicted line
        last_act = diag_df.iloc[-1]
        first_pre = pred_df.iloc[0]
        ax_p1.plot([last_act['í•™ë…„ë„'], first_pre['ì—°ë„']], [last_act['ê²½ìŸë¥ '], first_pre['ì˜ˆì¸¡ê²½ìŸë¥ ']], color='blue', linestyle='--', alpha=0.5)
        ax_p2.plot([last_act['í•™ë…„ë„'], first_pre['ì—°ë„']], [last_act['í‰ê· ì„±ì '], first_pre['ì˜ˆì¸¡í‰ê· ì„±ì ']], color='red', linestyle='--', alpha=0.5)
        
        ax_p1.set_ylabel("ê²½ìŸë¥  (:1)", color='blue', fontsize=12)
        ax_p2.set_ylabel("í‰ê·  ì„±ì  (ë“±ê¸‰)", color='red', fontsize=12)
        ax_p2.invert_yaxis()
        
        # Combined Legend
        lines1, labels1 = ax_p1.get_legend_handles_labels()
        lines2, labels2 = ax_p2.get_legend_handles_labels()
        ax_p2.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize='small')
        
        st.pyplot(fig_pred)

    col_tbl, col_msg = st.columns([1, 2])
    with col_tbl:
        st.markdown("##### ğŸ“‹ ì˜ˆì¸¡ ë°ì´í„° ìš”ì•½")
        disp_pred = pred_df[['ì—°ë„', 'ì˜ˆì¸¡ê²½ìŸë¥ ', 'ì˜ˆì¸¡í‰ê· ì„±ì ']].copy()
        disp_pred.columns = ['í•™ë…„ë„', 'ì˜ˆì¸¡ ê²½ìŸë¥ ', 'ì˜ˆì¸¡ í‰ê· ì„±ì ']
        st.dataframe(disp_pred.set_index('í•™ë…„ë„').style.format("{:.2f}"))
        
    with col_msg:
        final_comp = pred_df.iloc[-1]['ì˜ˆì¸¡ê²½ìŸë¥ ']
        final_grade = pred_df.iloc[-1]['ì˜ˆì¸¡í‰ê· ì„±ì ']
        st.warning(f"""
        **âš ï¸ {grp_name} ê²½ìŸë ¥ í•˜ë½ ê²½ê³ **: 2030ë…„ ì˜ˆìƒ ê²½ìŸë¥ ì€ **{final_comp:.2f}:1**ì´ë©°, í‰ê·  ì„±ì ì€ **{final_grade:.2f}ë“±ê¸‰**ê¹Œì§€ ë°€ë¦´ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. 
        í•´ë‹¹ ê·¸ë£¹({grp_name})ì— íŠ¹í™”ëœ ë¸Œëœë”© ë° ì „í˜• ìµœì í™” ì „ëµì´ í•„ìš”í•©ë‹ˆë‹¤.
        """)

st.markdown("---")

# --- 8. Comprehensive Strategic Recommendations (ğŸš€ ì…ì‹œ ì „ëµ ë¡œë“œë§µ) ---
st.header("ğŸš€ ì¢…í•© ì „ëµ ë° 2026 ì‹¤í–‰ ê³¼ì œ")

# Initialize default simulation values to prevent scope errors
sim_reductions_count = len(reductions) if 'reductions' in locals() else 0
sim_expansions_count = len(expansions) if 'expansions' in locals() else 0
sim_total_pool = total_pool if 'total_pool' in locals() else 0
sim_diff_val = diff_val if 'diff_val' in locals() else 0.0

col_rec1, col_rec2 = st.columns(2)

with col_rec1:
    st.subheader("ğŸ’¡ êµ¬ì¡° í˜ì‹ : 'ë²„ë¦´ ê³³'ì€ í™•ì‹¤íˆ, 'ë°€ì–´ì¤„ ê³³'ì€ ê°•ë ¥í•˜ê²Œ")
    st.success(f"""
    - **ì •ì› ë‹¤ì´ì–´íŠ¸**: ì„±ì ê³¼ ì¸ê¸°ê°€ ëª¨ë‘ ì—†ëŠ” **{sim_reductions_count}ê°œ** ì „í˜•ì€ ëŒ€í•™ì˜ ë¸Œëœë“œ ê°€ì¹˜ë¥¼ ë–¨ì–´ëœ¨ë¦¬ëŠ” 'ì•½í•œ ê³ ë¦¬'ì…ë‹ˆë‹¤. ì´ê³³ì—ì„œ **{sim_total_pool}ëª…**ì„ ê³¼ê°íˆ ì¤„ì—¬ì•¼ í•©ë‹ˆë‹¤.
    - **ì…ê²° ì í”„**: ë¶€ì¡±í•œ ì •ì›ì„ 'ìš°ìˆ˜ ì „í˜•'ìœ¼ë¡œ 100% ì˜®ê¸°ê¸°ë§Œ í•´ë„(ë¹„ìš© 0ì›!), ìš°ë¦¬ ëŒ€í•™ì˜ ì‹ ì…ìƒ í‰ê·  ìˆ˜ì¤€ì´ **{sim_diff_val:.3f}ë“±ê¸‰**ì´ë‚˜ ìˆ˜ì§ ìƒìŠ¹í•˜ëŠ” íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - **ì„ íƒê³¼ ì§‘ì¤‘**: ë“¤ì–´ì˜¤ëŠ” ë¬¸ì€ ì¢íˆë˜(ê°ì¶•), ê²½ìŸë ¥ì´ ì…ì¦ëœ í†µë¡œëŠ” ë„“í˜€ì„œ(ì¦ì›) **'ë“¤ì–´ê°€ê¸° ì–´ë ¤ìš´ ëŒ€í•™'**ì´ë¼ëŠ” ì´ë¯¸ì§€ë¥¼ êµ¬ì¶•í•˜ì‹­ì‹œì˜¤.
    """)

with col_rec2:
    st.subheader("ğŸ› ï¸ ë¯¸ë˜ ê³¼ì œ: ì¸êµ¬ ì ˆë²½ ì‹œëŒ€ì˜ ìƒì¡´ ì „ëµ")
    st.info(f"""
    - **ì„±ì¥íŒ ê³µëµ (ìš¸ì‚°/ì‹ ë„ì‹œ)**: ì¸êµ¬ê°€ ì¤„ì–´ë“œëŠ” êµ¬ë„ì‹¬ì€ ë°©ì–´ ìœ„ì£¼ë¡œ ê°€ë˜, êµìœ¡ ì—´ê¸°ê°€ ë†’ê³  ì¸êµ¬ ìœ ì…ì´ í™œë°œí•œ **ìš¸ì‚° ë° ì‹ ë„ì‹œ ê¶Œì—­**ì„ íƒ€ê²Ÿìœ¼ë¡œ ì§‘ì¤‘ì ì¸ í•™êµ í™ë³´ë¥¼ í¼ì³ì•¼ í•©ë‹ˆë‹¤.
    - **ì ì¬ë ¥ ì„ ë°œ (ë©´ì ‘ ê³ ë„í™”)**: ë‹¨ìˆœí•œ ë‚´ì‹  ì„±ì ë§Œìœ¼ë¡œëŠ” ì•Œ ìˆ˜ ì—†ëŠ” í•™ìƒì˜ 'ì§„ê°€'ë¥¼ ë°œê²¬í•˜ê¸° ìœ„í•´, ë‹¹ë½ ê²½ê³„ì„ (Borderline)ì— ìˆëŠ” í•™ìƒë“¤ì—ê²Œ ë©´ì ‘ ìì›ì„ ì§‘ì¤‘ íˆ¬ì…í•˜ì‹­ì‹œì˜¤.
    - **í•©ê²©ì ë§ˆìŒ ì¡ê¸° (Yield Management)**: "í•©ê²©ì€ ëì´ ì•„ë‹ˆë¼ ì‹œì‘ì…ë‹ˆë‹¤." ìµœì´ˆ í•©ê²©ìê°€ íƒ€ ëŒ€í•™ìœ¼ë¡œ ì´íƒˆí•˜ì§€ ì•Šë„ë¡, ì „ê³µ ì„ ë°°ì™€ì˜ ë§Œë‚¨ ë“± **ë°€ì°© ì¼€ì–´**ë¥¼ í†µí•´ ë“±ë¡ì„ í™•ì • ì§€ìœ¼ì‹­ì‹œì˜¤.
    """)

# --- Footer Summary Table ---
st.markdown("#### ğŸ“Š ë§ˆì§€ë§‰ ìš”ì  ì •ë¦¬ (Key Summary)")
df_pass_f = df_filtered.dropna(subset=['ëŒ€í‘œì„±ì '])
if not df_pass_f.empty:
    m_init = df_pass_f[df_pass_f['í•©ê²©êµ¬ë¶„'] == 'ìµœì´ˆí•©ê²©']['ëŒ€í‘œì„±ì '].mean()
    m_reg = df_pass_f[df_pass_f['ë“±ë¡êµ¬ë¶„'] == 'ë“±ë¡']['ëŒ€í‘œì„±ì '].mean()
    
    summary_data = {
        'êµ¬ë¶„': ['ìµœì´ˆí•©ê²©ì ìˆ˜ì¤€', 'ìµœì¢… ì…í•™ì ìˆ˜ì¤€', 'ì „ëµ ì„±ê³µ ì‹œ ì„±ì  í–¥ìƒì¹˜', 'ì§‘ì¤‘ ê³µëµ íƒ€ê²Ÿ ì§€ì—­'],
        'ì§€í‘œ': [f"{m_init:.2f} ë“±ê¸‰", f"{m_reg:.2f} ë“±ê¸‰", f"{sim_diff_val:.3f} ë“±ê¸‰ ìƒí–¥", "ìš¸ì‚° / ì‹ í¥ ì£¼ê±° ê¶Œì—­"],
    }
    st.table(pd.DataFrame(summary_data))
    st.caption("*ìµœì´ˆí•©ê²©ì ëŒ€ë¹„ ì…í•™ìì˜ ì„±ì ì´ ë‚®ì•„ì§€ëŠ” í˜„ìƒì€ 'ì´íƒˆ'ì— ì˜í•œ ê²ƒìœ¼ë¡œ, ìœ„ ë¡œë“œë§µì— ë”°ë¥¸ ë°€ì°© ì¼€ì–´ê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤.")
else:
    st.warning("ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ì–´ ìš”ì•½í‘œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
